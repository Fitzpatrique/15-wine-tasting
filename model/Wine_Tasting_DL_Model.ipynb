{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wine_Tasting_DL_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qVO73f_AwX9",
        "outputId": "1836de51-1554-4994-84f7-9cd0b0b1d7b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Mount your Google drive folder on Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9996Yahtdp7"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.stem import wordnet # to perform lemmitization\n",
        "#from sklearn.feature_extraction.text import CountVectorizer # to perform bow\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # to perform tfidf\n",
        "from nltk import pos_tag # for parts of speech\n",
        "from sklearn.metrics import pairwise_distances # to perform cosine similarity\n",
        "from nltk import word_tokenize #  to create tokens\n",
        "#from nltk.corpus import stopwords # for stop words"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWC-5Hx9P9TF",
        "outputId": "1b6fd9bf-5e81-4eea-a742-908d4cf9f12a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8t61oN5PbQB"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0xi20Xjk6kg"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB3gJ5IUlUMt"
      },
      "source": [
        "df = pd.read_csv('/content/gdrive/My Drive/wine_tasting_dataprep.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5gUsPKalfok"
      },
      "source": [
        "df.drop('Unnamed: 0', axis=1, inplace=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wj1nabxtTcC"
      },
      "source": [
        "variety_list = list(df['variety'].unique())\n",
        "winery_list = list(df['winery'].unique())\n",
        "location_list = list(df['Location'].unique())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4UdBX7mweDL",
        "outputId": "3ef69cff-bf34-4a9a-9578-5051d7e1b1c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "print('variety', len(variety_list))\n",
        "print('winery', len(winery_list))\n",
        "print('location', len(location_list))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "variety 750\n",
            "winery 19176\n",
            "location 1783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHectm--wrBB"
      },
      "source": [
        "def VID(text):\n",
        "  for i in range(len(variety_list)):\n",
        "    if text == variety_list[i]:\n",
        "      return i\n",
        "def WID(text):\n",
        "  for i in range(len(winery_list)):\n",
        "    if text == winery_list[i]:\n",
        "      return i\n",
        "def LID(text):\n",
        "  for i in range(len(location_list)):\n",
        "    if text == location_list[i]:\n",
        "      return i"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBUf3pi7w1Pn"
      },
      "source": [
        "df['variety_ID'] = df['variety'].apply(VID)\n",
        "df['winery_ID'] = df['winery'].apply(WID)\n",
        "df['location_ID'] =df['Location'].apply(LID)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ViaoFMTmAlj",
        "outputId": "5bed01b9-f65e-4624-cb75-1b7052e7cc5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "      <th>variety</th>\n",
              "      <th>winery</th>\n",
              "      <th>Location</th>\n",
              "      <th>lemmatized_text</th>\n",
              "      <th>variety_ID</th>\n",
              "      <th>winery_ID</th>\n",
              "      <th>location_ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
              "      <td>Cabernet Sauvignon</td>\n",
              "      <td>Heitz</td>\n",
              "      <td>Napa Valley California US</td>\n",
              "      <td>this tremendous varietal wine hail from oakvil...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ripe aromas of fig, blackberry and cassis are ...</td>\n",
              "      <td>Tinta de Toro</td>\n",
              "      <td>Bodega Carmen Rodríguez</td>\n",
              "      <td>Toro Northern Spain</td>\n",
              "      <td>ripe aroma of fig blackberry and cassis be sof...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mac Watson honors the memory of a wine once ma...</td>\n",
              "      <td>Sauvignon Blanc</td>\n",
              "      <td>Macauley</td>\n",
              "      <td>Knights Valley Sonoma California US</td>\n",
              "      <td>mac watson honor the memory of a wine once mak...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This spent 20 months in 30% new French oak, an...</td>\n",
              "      <td>Pinot Noir</td>\n",
              "      <td>Ponzi</td>\n",
              "      <td>Willamette Valley Oregon US</td>\n",
              "      <td>this spent month in new french oak and incorpo...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This is the top wine from La Bégude, named aft...</td>\n",
              "      <td>Provence red blend</td>\n",
              "      <td>Domaine de la Bégude</td>\n",
              "      <td>Bandol Provence France</td>\n",
              "      <td>this be the top wine from la b gude name after...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         description  ... location_ID\n",
              "0  This tremendous 100% varietal wine hails from ...  ...           0\n",
              "1  Ripe aromas of fig, blackberry and cassis are ...  ...           1\n",
              "2  Mac Watson honors the memory of a wine once ma...  ...           2\n",
              "3  This spent 20 months in 30% new French oak, an...  ...           3\n",
              "4  This is the top wine from La Bégude, named aft...  ...           4\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxx-9hGRoO4K"
      },
      "source": [
        "# using tf-idf\n",
        " \n",
        "tfidf = TfidfVectorizer(max_features=684,min_df=7, max_df=0.8, stop_words=stopwords.words('english')) # initializing tf-idf\n",
        "x_tfidf = tfidf.fit_transform(df['lemmatized_text']).toarray() # transforming the data to array"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exJ9MuM1porb"
      },
      "source": [
        "variety_id = np.array(list(df['variety_ID']))\n",
        "winery_id = np.array(list(df['winery_ID']))\n",
        "location_id = np.array(list(df['location_ID']))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JORFAm_ybyL"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Xv_train, Xv_test, yv_train, yv_test = train_test_split(x_tfidf, variety_id, test_size=0.2, random_state=101)\n",
        "Xw_train, Xw_test, yw_train, yw_test = train_test_split(x_tfidf, winery_id, test_size=0.2, random_state=101)\n",
        "Xl_train, Xl_test, yl_train, yl_test = train_test_split(x_tfidf, location_id, test_size=0.2, random_state=101)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saCJUC3szbkI",
        "outputId": "a311ba73-2322-4238-e33e-8d216984867b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "x_tfidf.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(410744, 684)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PXuYAIb26rK",
        "outputId": "03314b7e-40cd-4eaa-917b-bf39486d28d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# creating a model for predicting variety\n",
        "model_var  = tf.keras.Sequential([\n",
        "                tf.keras.layers.Flatten(input_shape=(1,684)),\n",
        "                tf.keras.layers.Dense(24, activation = 'relu'),\n",
        "                tf.keras.layers.Dense(750, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model_var.compile(optimizer = 'adam', loss='sparse_categorical_crossentropy', metrics =['accuracy'])\n",
        "\n",
        "\n",
        "num_epochs = 150\n",
        "\n",
        "history = model_var.fit(Xv_train, yv_train, epochs = num_epochs,\n",
        "                    validation_data = (Xv_test,yv_test))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 1, 684) for input Tensor(\"flatten_input:0\", shape=(None, 1, 684), dtype=float32), but it was called on an input with incompatible shape (None, 684).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 1, 684) for input Tensor(\"flatten_input:0\", shape=(None, 1, 684), dtype=float32), but it was called on an input with incompatible shape (None, 684).\n",
            "10249/10269 [============================>.] - ETA: 0s - loss: 2.5250 - accuracy: 0.4234WARNING:tensorflow:Model was constructed with shape (None, 1, 684) for input Tensor(\"flatten_input:0\", shape=(None, 1, 684), dtype=float32), but it was called on an input with incompatible shape (None, 684).\n",
            "10269/10269 [==============================] - 26s 3ms/step - loss: 2.5240 - accuracy: 0.4235 - val_loss: 2.0142 - val_accuracy: 0.5156\n",
            "Epoch 2/150\n",
            "10269/10269 [==============================] - 26s 3ms/step - loss: 1.9206 - accuracy: 0.5264 - val_loss: 1.8688 - val_accuracy: 0.5338\n",
            "Epoch 3/150\n",
            "10269/10269 [==============================] - 24s 2ms/step - loss: 1.8169 - accuracy: 0.5422 - val_loss: 1.8102 - val_accuracy: 0.5442\n",
            "Epoch 4/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.7651 - accuracy: 0.5488 - val_loss: 1.7796 - val_accuracy: 0.5483\n",
            "Epoch 5/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.7344 - accuracy: 0.5538 - val_loss: 1.7622 - val_accuracy: 0.5500\n",
            "Epoch 6/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.7134 - accuracy: 0.5566 - val_loss: 1.7499 - val_accuracy: 0.5517\n",
            "Epoch 7/150\n",
            "10269/10269 [==============================] - 24s 2ms/step - loss: 1.6984 - accuracy: 0.5584 - val_loss: 1.7410 - val_accuracy: 0.5529\n",
            "Epoch 8/150\n",
            "10269/10269 [==============================] - 24s 2ms/step - loss: 1.6874 - accuracy: 0.5593 - val_loss: 1.7368 - val_accuracy: 0.5539\n",
            "Epoch 9/150\n",
            "10269/10269 [==============================] - 28s 3ms/step - loss: 1.6782 - accuracy: 0.5608 - val_loss: 1.7305 - val_accuracy: 0.5533\n",
            "Epoch 10/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6714 - accuracy: 0.5606 - val_loss: 1.7269 - val_accuracy: 0.5537\n",
            "Epoch 11/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6650 - accuracy: 0.5623 - val_loss: 1.7255 - val_accuracy: 0.5544\n",
            "Epoch 12/150\n",
            "10269/10269 [==============================] - 24s 2ms/step - loss: 1.6602 - accuracy: 0.5622 - val_loss: 1.7214 - val_accuracy: 0.5550\n",
            "Epoch 13/150\n",
            "10269/10269 [==============================] - 26s 3ms/step - loss: 1.6560 - accuracy: 0.5631 - val_loss: 1.7199 - val_accuracy: 0.5549\n",
            "Epoch 14/150\n",
            "10269/10269 [==============================] - 26s 3ms/step - loss: 1.6520 - accuracy: 0.5635 - val_loss: 1.7200 - val_accuracy: 0.5554\n",
            "Epoch 15/150\n",
            "10269/10269 [==============================] - 24s 2ms/step - loss: 1.6489 - accuracy: 0.5641 - val_loss: 1.7179 - val_accuracy: 0.5551\n",
            "Epoch 16/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6459 - accuracy: 0.5644 - val_loss: 1.7138 - val_accuracy: 0.5564\n",
            "Epoch 17/150\n",
            "10269/10269 [==============================] - 24s 2ms/step - loss: 1.6430 - accuracy: 0.5652 - val_loss: 1.7170 - val_accuracy: 0.5557\n",
            "Epoch 18/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6410 - accuracy: 0.5654 - val_loss: 1.7149 - val_accuracy: 0.5564\n",
            "Epoch 19/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6387 - accuracy: 0.5651 - val_loss: 1.7153 - val_accuracy: 0.5553\n",
            "Epoch 20/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6367 - accuracy: 0.5659 - val_loss: 1.7149 - val_accuracy: 0.5553\n",
            "Epoch 21/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6355 - accuracy: 0.5664 - val_loss: 1.7150 - val_accuracy: 0.5543\n",
            "Epoch 22/150\n",
            "10269/10269 [==============================] - 29s 3ms/step - loss: 1.6336 - accuracy: 0.5660 - val_loss: 1.7107 - val_accuracy: 0.5557\n",
            "Epoch 23/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6324 - accuracy: 0.5663 - val_loss: 1.7110 - val_accuracy: 0.5570\n",
            "Epoch 24/150\n",
            "10269/10269 [==============================] - 24s 2ms/step - loss: 1.6309 - accuracy: 0.5664 - val_loss: 1.7098 - val_accuracy: 0.5566\n",
            "Epoch 25/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6297 - accuracy: 0.5662 - val_loss: 1.7116 - val_accuracy: 0.5562\n",
            "Epoch 26/150\n",
            "10269/10269 [==============================] - 26s 3ms/step - loss: 1.6287 - accuracy: 0.5665 - val_loss: 1.7114 - val_accuracy: 0.5570\n",
            "Epoch 27/150\n",
            "10269/10269 [==============================] - 24s 2ms/step - loss: 1.6279 - accuracy: 0.5668 - val_loss: 1.7096 - val_accuracy: 0.5578\n",
            "Epoch 28/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6269 - accuracy: 0.5666 - val_loss: 1.7114 - val_accuracy: 0.5564\n",
            "Epoch 29/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6261 - accuracy: 0.5672 - val_loss: 1.7086 - val_accuracy: 0.5570\n",
            "Epoch 30/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6251 - accuracy: 0.5676 - val_loss: 1.7093 - val_accuracy: 0.5581\n",
            "Epoch 31/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6242 - accuracy: 0.5680 - val_loss: 1.7097 - val_accuracy: 0.5574\n",
            "Epoch 32/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6233 - accuracy: 0.5678 - val_loss: 1.7086 - val_accuracy: 0.5578\n",
            "Epoch 33/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6228 - accuracy: 0.5679 - val_loss: 1.7094 - val_accuracy: 0.5579\n",
            "Epoch 34/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6220 - accuracy: 0.5682 - val_loss: 1.7101 - val_accuracy: 0.5579\n",
            "Epoch 35/150\n",
            "10269/10269 [==============================] - 30s 3ms/step - loss: 1.6216 - accuracy: 0.5680 - val_loss: 1.7079 - val_accuracy: 0.5576\n",
            "Epoch 36/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6208 - accuracy: 0.5681 - val_loss: 1.7086 - val_accuracy: 0.5575\n",
            "Epoch 37/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6205 - accuracy: 0.5680 - val_loss: 1.7102 - val_accuracy: 0.5583\n",
            "Epoch 38/150\n",
            "10269/10269 [==============================] - 26s 2ms/step - loss: 1.6199 - accuracy: 0.5682 - val_loss: 1.7094 - val_accuracy: 0.5577\n",
            "Epoch 39/150\n",
            "10269/10269 [==============================] - 26s 2ms/step - loss: 1.6192 - accuracy: 0.5687 - val_loss: 1.7098 - val_accuracy: 0.5584\n",
            "Epoch 40/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6186 - accuracy: 0.5690 - val_loss: 1.7077 - val_accuracy: 0.5586\n",
            "Epoch 41/150\n",
            "10269/10269 [==============================] - 24s 2ms/step - loss: 1.6183 - accuracy: 0.5686 - val_loss: 1.7094 - val_accuracy: 0.5591\n",
            "Epoch 42/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6180 - accuracy: 0.5690 - val_loss: 1.7080 - val_accuracy: 0.5588\n",
            "Epoch 43/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6174 - accuracy: 0.5691 - val_loss: 1.7062 - val_accuracy: 0.5589\n",
            "Epoch 44/150\n",
            "10269/10269 [==============================] - 26s 2ms/step - loss: 1.6172 - accuracy: 0.5691 - val_loss: 1.7095 - val_accuracy: 0.5586\n",
            "Epoch 45/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6168 - accuracy: 0.5693 - val_loss: 1.7092 - val_accuracy: 0.5597\n",
            "Epoch 46/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6163 - accuracy: 0.5693 - val_loss: 1.7104 - val_accuracy: 0.5580\n",
            "Epoch 47/150\n",
            "10269/10269 [==============================] - 27s 3ms/step - loss: 1.6159 - accuracy: 0.5693 - val_loss: 1.7093 - val_accuracy: 0.5591\n",
            "Epoch 48/150\n",
            "10269/10269 [==============================] - 27s 3ms/step - loss: 1.6156 - accuracy: 0.5695 - val_loss: 1.7089 - val_accuracy: 0.5591\n",
            "Epoch 49/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6151 - accuracy: 0.5695 - val_loss: 1.7080 - val_accuracy: 0.5582\n",
            "Epoch 50/150\n",
            "10269/10269 [==============================] - 26s 2ms/step - loss: 1.6149 - accuracy: 0.5695 - val_loss: 1.7072 - val_accuracy: 0.5584\n",
            "Epoch 51/150\n",
            "10269/10269 [==============================] - 26s 3ms/step - loss: 1.6143 - accuracy: 0.5696 - val_loss: 1.7103 - val_accuracy: 0.5592\n",
            "Epoch 52/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6143 - accuracy: 0.5694 - val_loss: 1.7104 - val_accuracy: 0.5588\n",
            "Epoch 53/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6140 - accuracy: 0.5691 - val_loss: 1.7089 - val_accuracy: 0.5599\n",
            "Epoch 54/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6135 - accuracy: 0.5697 - val_loss: 1.7109 - val_accuracy: 0.5594\n",
            "Epoch 55/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6135 - accuracy: 0.5700 - val_loss: 1.7091 - val_accuracy: 0.5585\n",
            "Epoch 56/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6132 - accuracy: 0.5700 - val_loss: 1.7112 - val_accuracy: 0.5598\n",
            "Epoch 57/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6131 - accuracy: 0.5696 - val_loss: 1.7110 - val_accuracy: 0.5589\n",
            "Epoch 58/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6128 - accuracy: 0.5697 - val_loss: 1.7117 - val_accuracy: 0.5583\n",
            "Epoch 59/150\n",
            "10269/10269 [==============================] - 24s 2ms/step - loss: 1.6126 - accuracy: 0.5699 - val_loss: 1.7094 - val_accuracy: 0.5592\n",
            "Epoch 60/150\n",
            "10269/10269 [==============================] - 27s 3ms/step - loss: 1.6125 - accuracy: 0.5696 - val_loss: 1.7100 - val_accuracy: 0.5601\n",
            "Epoch 61/150\n",
            "10269/10269 [==============================] - 27s 3ms/step - loss: 1.6120 - accuracy: 0.5700 - val_loss: 1.7094 - val_accuracy: 0.5603\n",
            "Epoch 62/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6120 - accuracy: 0.5694 - val_loss: 1.7100 - val_accuracy: 0.5585\n",
            "Epoch 63/150\n",
            "10269/10269 [==============================] - 27s 3ms/step - loss: 1.6120 - accuracy: 0.5701 - val_loss: 1.7107 - val_accuracy: 0.5579\n",
            "Epoch 64/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6117 - accuracy: 0.5704 - val_loss: 1.7084 - val_accuracy: 0.5607\n",
            "Epoch 65/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6117 - accuracy: 0.5700 - val_loss: 1.7093 - val_accuracy: 0.5598\n",
            "Epoch 66/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6113 - accuracy: 0.5694 - val_loss: 1.7114 - val_accuracy: 0.5583\n",
            "Epoch 67/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6112 - accuracy: 0.5700 - val_loss: 1.7107 - val_accuracy: 0.5610\n",
            "Epoch 68/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6114 - accuracy: 0.5700 - val_loss: 1.7119 - val_accuracy: 0.5589\n",
            "Epoch 69/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6112 - accuracy: 0.5697 - val_loss: 1.7123 - val_accuracy: 0.5596\n",
            "Epoch 70/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6112 - accuracy: 0.5699 - val_loss: 1.7112 - val_accuracy: 0.5600\n",
            "Epoch 71/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6104 - accuracy: 0.5695 - val_loss: 1.7146 - val_accuracy: 0.5588\n",
            "Epoch 72/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6108 - accuracy: 0.5705 - val_loss: 1.7126 - val_accuracy: 0.5599\n",
            "Epoch 73/150\n",
            "10269/10269 [==============================] - 29s 3ms/step - loss: 1.6108 - accuracy: 0.5697 - val_loss: 1.7127 - val_accuracy: 0.5599\n",
            "Epoch 74/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6103 - accuracy: 0.5702 - val_loss: 1.7120 - val_accuracy: 0.5586\n",
            "Epoch 75/150\n",
            "10269/10269 [==============================] - 27s 3ms/step - loss: 1.6104 - accuracy: 0.5700 - val_loss: 1.7123 - val_accuracy: 0.5594\n",
            "Epoch 76/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6101 - accuracy: 0.5705 - val_loss: 1.7148 - val_accuracy: 0.5597\n",
            "Epoch 77/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6102 - accuracy: 0.5702 - val_loss: 1.7119 - val_accuracy: 0.5590\n",
            "Epoch 78/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6103 - accuracy: 0.5703 - val_loss: 1.7124 - val_accuracy: 0.5585\n",
            "Epoch 79/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6101 - accuracy: 0.5697 - val_loss: 1.7122 - val_accuracy: 0.5597\n",
            "Epoch 80/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6100 - accuracy: 0.5706 - val_loss: 1.7125 - val_accuracy: 0.5585\n",
            "Epoch 81/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6100 - accuracy: 0.5702 - val_loss: 1.7113 - val_accuracy: 0.5590\n",
            "Epoch 82/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6096 - accuracy: 0.5705 - val_loss: 1.7130 - val_accuracy: 0.5596\n",
            "Epoch 83/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6096 - accuracy: 0.5703 - val_loss: 1.7170 - val_accuracy: 0.5598\n",
            "Epoch 84/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6098 - accuracy: 0.5699 - val_loss: 1.7143 - val_accuracy: 0.5590\n",
            "Epoch 85/150\n",
            "10269/10269 [==============================] - 26s 3ms/step - loss: 1.6093 - accuracy: 0.5703 - val_loss: 1.7123 - val_accuracy: 0.5602\n",
            "Epoch 86/150\n",
            "10269/10269 [==============================] - 29s 3ms/step - loss: 1.6094 - accuracy: 0.5704 - val_loss: 1.7150 - val_accuracy: 0.5605\n",
            "Epoch 87/150\n",
            "10269/10269 [==============================] - 27s 3ms/step - loss: 1.6094 - accuracy: 0.5704 - val_loss: 1.7133 - val_accuracy: 0.5594\n",
            "Epoch 88/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6093 - accuracy: 0.5705 - val_loss: 1.7153 - val_accuracy: 0.5595\n",
            "Epoch 89/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6095 - accuracy: 0.5704 - val_loss: 1.7155 - val_accuracy: 0.5588\n",
            "Epoch 90/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6094 - accuracy: 0.5700 - val_loss: 1.7133 - val_accuracy: 0.5593\n",
            "Epoch 91/150\n",
            "10269/10269 [==============================] - 26s 2ms/step - loss: 1.6092 - accuracy: 0.5701 - val_loss: 1.7153 - val_accuracy: 0.5588\n",
            "Epoch 92/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6092 - accuracy: 0.5702 - val_loss: 1.7145 - val_accuracy: 0.5591\n",
            "Epoch 93/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6089 - accuracy: 0.5702 - val_loss: 1.7148 - val_accuracy: 0.5604\n",
            "Epoch 94/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6091 - accuracy: 0.5707 - val_loss: 1.7156 - val_accuracy: 0.5595\n",
            "Epoch 95/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6089 - accuracy: 0.5705 - val_loss: 1.7150 - val_accuracy: 0.5591\n",
            "Epoch 96/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6091 - accuracy: 0.5707 - val_loss: 1.7163 - val_accuracy: 0.5593\n",
            "Epoch 97/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6091 - accuracy: 0.5705 - val_loss: 1.7160 - val_accuracy: 0.5593\n",
            "Epoch 98/150\n",
            "10269/10269 [==============================] - 29s 3ms/step - loss: 1.6089 - accuracy: 0.5708 - val_loss: 1.7156 - val_accuracy: 0.5591\n",
            "Epoch 99/150\n",
            "10269/10269 [==============================] - 28s 3ms/step - loss: 1.6089 - accuracy: 0.5703 - val_loss: 1.7185 - val_accuracy: 0.5589\n",
            "Epoch 100/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6090 - accuracy: 0.5704 - val_loss: 1.7146 - val_accuracy: 0.5598\n",
            "Epoch 101/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6091 - accuracy: 0.5708 - val_loss: 1.7180 - val_accuracy: 0.5592\n",
            "Epoch 102/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6091 - accuracy: 0.5703 - val_loss: 1.7168 - val_accuracy: 0.5588\n",
            "Epoch 103/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6088 - accuracy: 0.5703 - val_loss: 1.7183 - val_accuracy: 0.5580\n",
            "Epoch 104/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6088 - accuracy: 0.5704 - val_loss: 1.7168 - val_accuracy: 0.5581\n",
            "Epoch 105/150\n",
            "10269/10269 [==============================] - 26s 3ms/step - loss: 1.6087 - accuracy: 0.5702 - val_loss: 1.7153 - val_accuracy: 0.5595\n",
            "Epoch 106/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6088 - accuracy: 0.5700 - val_loss: 1.7164 - val_accuracy: 0.5596\n",
            "Epoch 107/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6087 - accuracy: 0.5703 - val_loss: 1.7166 - val_accuracy: 0.5598\n",
            "Epoch 108/150\n",
            "10269/10269 [==============================] - 26s 2ms/step - loss: 1.6087 - accuracy: 0.5702 - val_loss: 1.7173 - val_accuracy: 0.5577\n",
            "Epoch 109/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6085 - accuracy: 0.5704 - val_loss: 1.7190 - val_accuracy: 0.5593\n",
            "Epoch 110/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6088 - accuracy: 0.5701 - val_loss: 1.7179 - val_accuracy: 0.5595\n",
            "Epoch 111/150\n",
            "10269/10269 [==============================] - 32s 3ms/step - loss: 1.6086 - accuracy: 0.5701 - val_loss: 1.7183 - val_accuracy: 0.5592\n",
            "Epoch 112/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6087 - accuracy: 0.5701 - val_loss: 1.7207 - val_accuracy: 0.5587\n",
            "Epoch 113/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6084 - accuracy: 0.5707 - val_loss: 1.7200 - val_accuracy: 0.5572\n",
            "Epoch 114/150\n",
            "10269/10269 [==============================] - 26s 3ms/step - loss: 1.6086 - accuracy: 0.5697 - val_loss: 1.7194 - val_accuracy: 0.5599\n",
            "Epoch 115/150\n",
            "10269/10269 [==============================] - 26s 3ms/step - loss: 1.6089 - accuracy: 0.5706 - val_loss: 1.7188 - val_accuracy: 0.5595\n",
            "Epoch 116/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6088 - accuracy: 0.5705 - val_loss: 1.7207 - val_accuracy: 0.5589\n",
            "Epoch 117/150\n",
            "10269/10269 [==============================] - 26s 3ms/step - loss: 1.6085 - accuracy: 0.5707 - val_loss: 1.7183 - val_accuracy: 0.5593\n",
            "Epoch 118/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6084 - accuracy: 0.5702 - val_loss: 1.7190 - val_accuracy: 0.5592\n",
            "Epoch 119/150\n",
            "10269/10269 [==============================] - 26s 2ms/step - loss: 1.6088 - accuracy: 0.5702 - val_loss: 1.7189 - val_accuracy: 0.5595\n",
            "Epoch 120/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6085 - accuracy: 0.5703 - val_loss: 1.7192 - val_accuracy: 0.5593\n",
            "Epoch 121/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6084 - accuracy: 0.5708 - val_loss: 1.7195 - val_accuracy: 0.5593\n",
            "Epoch 122/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6085 - accuracy: 0.5700 - val_loss: 1.7197 - val_accuracy: 0.5595\n",
            "Epoch 123/150\n",
            "10269/10269 [==============================] - 28s 3ms/step - loss: 1.6087 - accuracy: 0.5707 - val_loss: 1.7209 - val_accuracy: 0.5580\n",
            "Epoch 124/150\n",
            "10269/10269 [==============================] - 27s 3ms/step - loss: 1.6089 - accuracy: 0.5707 - val_loss: 1.7220 - val_accuracy: 0.5585\n",
            "Epoch 125/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6086 - accuracy: 0.5705 - val_loss: 1.7212 - val_accuracy: 0.5595\n",
            "Epoch 126/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6087 - accuracy: 0.5706 - val_loss: 1.7217 - val_accuracy: 0.5582\n",
            "Epoch 127/150\n",
            "10269/10269 [==============================] - 25s 2ms/step - loss: 1.6089 - accuracy: 0.5705 - val_loss: 1.7213 - val_accuracy: 0.5590\n",
            "Epoch 128/150\n",
            "10269/10269 [==============================] - 36s 4ms/step - loss: 1.6090 - accuracy: 0.5701 - val_loss: 1.7215 - val_accuracy: 0.5591\n",
            "Epoch 129/150\n",
            "10269/10269 [==============================] - 26s 3ms/step - loss: 1.6087 - accuracy: 0.5707 - val_loss: 1.7211 - val_accuracy: 0.5585\n",
            "Epoch 130/150\n",
            "10269/10269 [==============================] - 26s 3ms/step - loss: 1.6089 - accuracy: 0.5704 - val_loss: 1.7216 - val_accuracy: 0.5596\n",
            "Epoch 131/150\n",
            "10269/10269 [==============================] - 26s 2ms/step - loss: 1.6090 - accuracy: 0.5704 - val_loss: 1.7215 - val_accuracy: 0.5592\n",
            "Epoch 132/150\n",
            " 2012/10269 [====>.........................] - ETA: 18s - loss: 1.5926 - accuracy: 0.5724Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jloVRc9C5ufH"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}